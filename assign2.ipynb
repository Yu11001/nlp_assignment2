{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\24746\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\24746\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\24746\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "# type: ignore\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import time\n",
    "\n",
    "from textstat import flesch_kincaid_grade, flesch_reading_ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "train_data = pd.read_csv('data/train.csv', encoding='Windows-1252')\n",
    "test_data = pd.read_csv('data/test.csv', encoding='Windows-1252')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set columns' name\n",
    "train_data.columns = ['sentiment','id','date','query','user_id','text']\n",
    "test_data.columns = ['sentiment','id','date','query','user_id','text']\n",
    "\n",
    "# train_data\n",
    "# test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment\n",
    "train_data = train_data[train_data.sentiment.isin([0, 1, 2, 3, 4])]\n",
    "\n",
    "train_data.drop(['id','date','query','user_id'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count the number of documents: 1599999\n",
      "Average tokens per document: 16.410284631427896\n",
      "Total vocabulary size: 874870\n"
     ]
    }
   ],
   "source": [
    "# Count the number of documents\n",
    "num_documents1 = len(train_data)\n",
    "print(f\"Count the number of documents: {num_documents1}\")\n",
    "\n",
    "# Average tokens per document\n",
    "train_data['token_count'] = train_data['text'].apply(lambda x: len(word_tokenize(x)))\n",
    "average_tokens1 = train_data['token_count'].mean()\n",
    "print(f\"Average tokens per document: {average_tokens1}\")\n",
    "\n",
    "# Total vocabulary size\n",
    "vocabulary = set(word for text in train_data['text'] for word in word_tokenize(text))\n",
    "vocabulary_size1 = len(vocabulary)\n",
    "print(f\"Total vocabulary size: {vocabulary_size1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove emoticons and emojis\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize and remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join(word for word in word_tokenize(text) if word not in stop_words)\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = ' '.join(lemmatizer.lemmatize(word) for word in text.split())\n",
    "\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    text = ' '.join(stemmer.stem(word) for word in text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "train_data['cleaned_text'] = train_data['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>token_count</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>25</td>\n",
       "      <td>upset cant updat facebook text might cri resul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>21</td>\n",
       "      <td>kenichan dive mani time ball manag save rest g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>10</td>\n",
       "      <td>whole bodi feel itchi like fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>30</td>\n",
       "      <td>nationwideclass behav im mad cant see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "      <td>6</td>\n",
       "      <td>kwesidei whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599994</th>\n",
       "      <td>4</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "      <td>12</td>\n",
       "      <td>woke school best feel ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "      <td>15</td>\n",
       "      <td>thewdbcom cool hear old walt interview httpbli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "      <td>12</td>\n",
       "      <td>readi mojo makeov ask detail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "      <td>15</td>\n",
       "      <td>happi th birthday boo alll time tupac amaru sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "      <td>9</td>\n",
       "      <td>happi charitytuesday thenspcc sparkschar speak...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599999 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment                                               text  \\\n",
       "0                0  is upset that he can't update his Facebook by ...   \n",
       "1                0  @Kenichan I dived many times for the ball. Man...   \n",
       "2                0    my whole body feels itchy and like its on fire    \n",
       "3                0  @nationwideclass no, it's not behaving at all....   \n",
       "4                0                      @Kwesidei not the whole crew    \n",
       "...            ...                                                ...   \n",
       "1599994          4  Just woke up. Having no school is the best fee...   \n",
       "1599995          4  TheWDB.com - Very cool to hear old Walt interv...   \n",
       "1599996          4  Are you ready for your MoJo Makeover? Ask me f...   \n",
       "1599997          4  Happy 38th Birthday to my boo of alll time!!! ...   \n",
       "1599998          4  happy #charitytuesday @theNSPCC @SparksCharity...   \n",
       "\n",
       "         token_count                                       cleaned_text  \n",
       "0                 25  upset cant updat facebook text might cri resul...  \n",
       "1                 21  kenichan dive mani time ball manag save rest g...  \n",
       "2                 10                    whole bodi feel itchi like fire  \n",
       "3                 30              nationwideclass behav im mad cant see  \n",
       "4                  6                                kwesidei whole crew  \n",
       "...              ...                                                ...  \n",
       "1599994           12                         woke school best feel ever  \n",
       "1599995           15  thewdbcom cool hear old walt interview httpbli...  \n",
       "1599996           12                       readi mojo makeov ask detail  \n",
       "1599997           15  happi th birthday boo alll time tupac amaru sh...  \n",
       "1599998            9  happi charitytuesday thenspcc sparkschar speak...  \n",
       "\n",
       "[1599999 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count the number of documents: 1599999 -> 1599999\n",
      "Average tokens per document: 16.410284631427896 -> 7.715041696901061\n",
      "Total vocabulary size: 874870 -> 726859\n",
      "Number of stop words removed: 8737568\n",
      "Number of special characters removed: 12682030\n",
      "Number of empty documents after cleaning: 391\n",
      "Total runtime: 1505.4705233573914 seconds\n"
     ]
    }
   ],
   "source": [
    "# Count the number of documents\n",
    "num_documents2 = len(train_data)\n",
    "print(f\"Count the number of documents: {num_documents1} -> {num_documents2}\")\n",
    "\n",
    "# Average tokens per document\n",
    "train_data['token_count'] = train_data['cleaned_text'].apply(lambda x: len(word_tokenize(x)))\n",
    "average_tokens2 = train_data['token_count'].mean()\n",
    "print(f\"Average tokens per document: {average_tokens1} -> {average_tokens2}\")\n",
    "\n",
    "# Total vocabulary size\n",
    "vocabulary = set(word for text in train_data['cleaned_text'] for word in word_tokenize(text))\n",
    "vocabulary_size2 = len(vocabulary)\n",
    "print(f\"Total vocabulary size: {vocabulary_size1} -> {vocabulary_size2}\")\n",
    "\n",
    "# Number of stop words removed\n",
    "stop_words = set(stopwords.words('english'))\n",
    "original_stopword_count = train_data['text'].apply(lambda x: sum(1 for word in word_tokenize(x) if word.lower() in stop_words)).sum()\n",
    "cleaned_stopword_count = train_data['cleaned_text'].apply(lambda x: sum(1 for word in word_tokenize(x) if word.lower() in stop_words)).sum()\n",
    "stopwords_removed = original_stopword_count - cleaned_stopword_count\n",
    "print(f\"Number of stop words removed: {stopwords_removed}\")\n",
    "\n",
    "\n",
    "# Number of special characters removed\n",
    "def count_special_characters(text):\n",
    "    return len(re.findall(r'[^\\x00-\\x7F]', text)) + len(re.findall(r'[^a-z\\s]', text))\n",
    "original_special_char_count = train_data['text'].apply(count_special_characters).sum()\n",
    "cleaned_special_char_count = train_data['cleaned_text'].apply(count_special_characters).sum()\n",
    "special_chars_removed = original_special_char_count - cleaned_special_char_count\n",
    "print(f\"Number of special characters removed: {special_chars_removed}\")\n",
    "\n",
    "# Count empty documents after cleaning\n",
    "empty_documents_after_cleaning = train_data['cleaned_text'].apply(lambda x: len(x.strip()) == 0).sum()\n",
    "print(f\"Number of empty documents after cleaning: {empty_documents_after_cleaning}\")\n",
    "\n",
    "end = time.time()\n",
    "runtime = end - start\n",
    "print(f\"Total runtime: {runtime} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Flesch-Kincaid Grade Level: 4.526849640661964\n",
      "Average Flesch Reading Ease Score: 76.32139612330023\n",
      "Lexical Diversity (unique/total words): 0.05888330572626071\n"
     ]
    }
   ],
   "source": [
    "# Readability Scores\n",
    "def readability_scores(text_column):\n",
    "    fk_grades = text_column.apply(lambda x: flesch_kincaid_grade(x) if len(x.strip()) > 0 else None)\n",
    "    fk_reading_ease = text_column.apply(lambda x: flesch_reading_ease(x) if len(x.strip()) > 0 else None)\n",
    "    avg_fk_grade = fk_grades.mean()\n",
    "    avg_fk_reading_ease = fk_reading_ease.mean()\n",
    "    return avg_fk_grade, avg_fk_reading_ease\n",
    "\n",
    "# Lexical Diversity\n",
    "def lexical_diversity(text_column):\n",
    "    total_words = text_column.apply(lambda x: len(word_tokenize(x))).sum()\n",
    "    unique_words = len(set(word for text in text_column for word in word_tokenize(text)))\n",
    "    return unique_words / total_words if total_words > 0 else 0\n",
    "\n",
    "# Evaluate cleaned text\n",
    "avg_fk_grade, avg_fk_reading_ease = readability_scores(train_data['cleaned_text'])\n",
    "lexical_div = lexical_diversity(train_data['cleaned_text'])\n",
    "\n",
    "print(f\"Average Flesch-Kincaid Grade Level: {avg_fk_grade}\")\n",
    "print(f\"Average Flesch Reading Ease Score: {avg_fk_reading_ease}\")\n",
    "print(f\"Lexical Diversity (unique/total words): {lexical_div}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red;\">Results</h1>\n",
    "<img src=\"data/image.png\">\n",
    "<img src=\"data/image2.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
